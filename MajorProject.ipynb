{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MajorProject.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOQHZuaoxb2lOfYFcVH8WNE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4wSKXLtCbZOD","executionInfo":{"status":"ok","timestamp":1609227414313,"user_tz":-330,"elapsed":1807,"user":{"displayName":"Anushka Mishra","photoUrl":"","userId":"01080016831886281264"}},"outputId":"46a9f6fa-ccc3-4665-b6d2-e26f81020f51"},"source":["import nltk\r\n","from nltk.stem.wordnet import WordNetLemmatizer\r\n","from nltk.corpus import twitter_samples\r\n","nltk.download('stopwords')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","nltk.download('wordnet')\r\n","from nltk.tag import pos_tag\r\n","from nltk.tokenize import word_tokenize\r\n","from nltk import FreqDist, classify, NaiveBayesClassifier\r\n","\r\n","import re, string, random"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EdATXMoWYdh4","executionInfo":{"status":"ok","timestamp":1609227510458,"user_tz":-330,"elapsed":1531,"user":{"displayName":"Anushka Mishra","photoUrl":"","userId":"01080016831886281264"}}},"source":["\r\n","\r\n","def remove_noise(tweet_tokens, stop_words = ()):\r\n","\r\n","    cleaned_tokens = []\r\n","\r\n","    for token, tag in pos_tag(tweet_tokens):\r\n","        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\r\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\r\n","        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\r\n","\r\n","        if tag.startswith(\"NN\"):\r\n","            pos = 'n'\r\n","        elif tag.startswith('VB'):\r\n","            pos = 'v'\r\n","        else:\r\n","            pos = 'a'\r\n","\r\n","        lemmatizer = WordNetLemmatizer()\r\n","        token = lemmatizer.lemmatize(token, pos)\r\n","\r\n","        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\r\n","            cleaned_tokens.append(token.lower())\r\n","    return cleaned_tokens\r\n","\r\n","def get_all_words(cleaned_tokens_list):\r\n","    for tokens in cleaned_tokens_list:\r\n","        for token in tokens:\r\n","            yield token\r\n","\r\n","def get_tweets_for_model(cleaned_tokens_list):\r\n","    for tweet_tokens in cleaned_tokens_list:\r\n","        yield dict([token, True] for token in tweet_tokens)\r\n","   \r\n","    "],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KwMV5C3blb1","executionInfo":{"status":"ok","timestamp":1609227532163,"user_tz":-330,"elapsed":14477,"user":{"displayName":"Anushka Mishra","photoUrl":"","userId":"01080016831886281264"}},"outputId":"e4fe01ee-f10d-48d8-dd99-414761942ed6"},"source":["    positive_tweets = twitter_samples.strings('positive_tweets.json')\r\n","    negative_tweets = twitter_samples.strings('negative_tweets.json')\r\n","    text = twitter_samples.strings('tweets.20150430-223406.json')\r\n","    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\r\n","\r\n","    stop_words = stopwords.words('english')\r\n","\r\n","    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\r\n","    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\r\n","\r\n","    positive_cleaned_tokens_list = []\r\n","    negative_cleaned_tokens_list = []\r\n","\r\n","    for tokens in positive_tweet_tokens:\r\n","        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\r\n","\r\n","    for tokens in negative_tweet_tokens:\r\n","        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\r\n","\r\n","    all_pos_words = get_all_words(positive_cleaned_tokens_list)\r\n","\r\n","    freq_dist_pos = FreqDist(all_pos_words)\r\n","    print(freq_dist_pos.most_common(10))\r\n","\r\n","    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\r\n","    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\r\n","\r\n","    positive_dataset = [(tweet_dict, \"Positive\")\r\n","                         for tweet_dict in positive_tokens_for_model]\r\n","\r\n","    negative_dataset = [(tweet_dict, \"Negative\")\r\n","                         for tweet_dict in negative_tokens_for_model]\r\n","\r\n","    dataset = positive_dataset + negative_dataset\r\n","\r\n","    random.shuffle(dataset)\r\n","\r\n","    train_data = dataset[:7000]\r\n","    test_data = dataset[7000:]\r\n","\r\n","    classifier = NaiveBayesClassifier.train(train_data)\r\n","\r\n","    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\r\n","\r\n","    print(classifier.show_most_informative_features(10))\r\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n","Accuracy is: 0.994\n","Most Informative Features\n","                      :( = True           Negati : Positi =   2056.7 : 1.0\n","                      :) = True           Positi : Negati =    998.9 : 1.0\n","                follower = True           Positi : Negati =     24.4 : 1.0\n","                followed = True           Negati : Positi =     24.2 : 1.0\n","                    glad = True           Positi : Negati =     19.8 : 1.0\n","                     sad = True           Negati : Positi =     19.3 : 1.0\n","                 welcome = True           Positi : Negati =     16.0 : 1.0\n","                  arrive = True           Positi : Negati =     14.5 : 1.0\n","                    miss = True           Negati : Positi =     12.4 : 1.0\n","               community = True           Positi : Negati =     12.4 : 1.0\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpDZHBMnYoo5","executionInfo":{"status":"ok","timestamp":1609227606016,"user_tz":-330,"elapsed":1186,"user":{"displayName":"Anushka Mishra","photoUrl":"","userId":"01080016831886281264"}},"outputId":"f42dea61-e22c-4d35-ad43-d0adda63a2a4"},"source":["custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\r\n","\r\n","custom_tokens = remove_noise(word_tokenize(custom_tweet))\r\n","\r\n","print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KPkblmp7YyAO"},"source":[""],"execution_count":null,"outputs":[]}]}